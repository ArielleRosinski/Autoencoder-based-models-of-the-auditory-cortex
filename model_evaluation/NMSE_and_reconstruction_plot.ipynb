{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycochleagram.cochleagram as cgram\n",
    "from pycochleagram import utils\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import os \n",
    "from os import listdir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sound_dataset import SoundDataset_kfold\n",
    "from AE_architectures import AE_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get NMSE and plot reconstructions for a given trained model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/ariellerosinski/My Drive/UCL/MSc/Project/Code/full_coch\" #path to datasets (needs to be changed)\n",
    "\n",
    "sound_files=listdir(path)\n",
    "\n",
    "file_name=[]\n",
    "files=[]\n",
    "for file in sound_files:\n",
    "    if file.endswith(\".npy\") and file.startswith(\"full\") and file != \"full_forest_coch.npy\":\n",
    "        file_name.append(file)\n",
    "        full_path=[path,file]\n",
    "        full_path=\"/\".join(full_path)\n",
    "        files.append(full_path)\n",
    "\n",
    "k_folds=5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=False)\n",
    "foldid=0\n",
    "\n",
    "\n",
    "def get_windows(files, kfold, fold_id, file_name, split_size=100, norm=True, training=True):\n",
    "    ls_coch=[]\n",
    "    ls_file=[]\n",
    "    ls_file_name=[]\n",
    "    for i,file in enumerate(files): \n",
    "        file=np.load(file)\n",
    "\n",
    "        for fold, (train_ids, test_ids) in enumerate(kfold.split(file.T)):\n",
    "            if fold==fold_id:\n",
    "                if norm==True:\n",
    "                    if file_name[i].endswith(\"coch.npy\"):\n",
    "                        file=file/2500\n",
    "                    elif file_name[i] == \"full_pennington_david.npy\":\n",
    "                        file=file/1000\n",
    "                    else: \n",
    "                        file=file/5000\n",
    "\n",
    "                if training:\n",
    "                    indices=train_ids\n",
    "                else:\n",
    "                    indices=test_ids\n",
    "                \n",
    "                file=file.T[indices].T\n",
    "\n",
    "    \n",
    "                split_start=0\n",
    "                split_end=split_start+split_size\n",
    "                for j in range(file.shape[1]//split_size):\n",
    "                    window=file[:,split_start:split_end]\n",
    "                    split_start += split_size\n",
    "                    split_end += split_size\n",
    "                    ls_coch.append(window)\n",
    "                         \n",
    "    return ls_coch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation for save, trained model (need to alter save paths)\n",
    "loss_function=nn.MSELoss()\n",
    "\n",
    "hidden_size=32\n",
    "time_lag=99\n",
    "burn_in=0\n",
    "foldid=0\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "test_data=SoundDataset_kfold(get_windows(files, kfold, foldid, file_name, norm=True, training=False), transform = transforms.Compose([transforms.ToTensor(),]),uint8=False ) \n",
    "loader = DataLoader(test_data,batch_size=1,shuffle=False) \n",
    "\n",
    "model = AE_RNN(time_lag=time_lag, burn_in=burn_in, hidden_size=hidden_size).float() \n",
    "model_path=\"/path_to_.pt_model\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "ls_image = []\n",
    "ls_reconstructed = []\n",
    "\n",
    "for i,batch in enumerate(loader):\n",
    "    image = batch[0].float() \n",
    "\n",
    "    initialization=torch.zeros((1,image.shape[0],hidden_size)) \n",
    "                    \n",
    "    reconstructed=model.forward_train(image.float(),initialization,device) \n",
    "                             \n",
    "    x=torch.swapaxes(image[:,0,:,:],1,2)                \n",
    "    x_list = [x[:,time_lag+burn_in:,:]]\n",
    "    for i in range(time_lag-1):\n",
    "        x_t_minus_i=x[:,time_lag+burn_in-(i+1):-(i+1),:]\n",
    "        x_list.append(x_t_minus_i)                                        \n",
    "                \n",
    "    img_comparison=torch.stack(x_list,axis=2) \n",
    "\n",
    "\n",
    "    ls_image.append(image)\n",
    "    ls_reconstructed.append(reconstructed.detach()) \n",
    "\n",
    "image_all=np.vstack(ls_image[:-1]).squeeze() \n",
    "reconstructed_all=np.vstack(ls_reconstructed[:-1])\n",
    "reconstructed_all_flipped=np.flip(reconstructed_all,axis=2)                     #flip because autoencoder reconstructs from most to least recent versus original image (least to more recent)\n",
    "reconstructed_comp=np.swapaxes(reconstructed_all_flipped, -1, -2).squeeze()\n",
    "\n",
    "#Normalize by the mean squares across all cochleagram in the dataset\n",
    "MSE=((image_all[:,:,-30:]-reconstructed_comp[:,:,-30:])**2).mean() \n",
    "norm = (image_all[:,:,-30:]**2).mean()\n",
    "mse_norm = MSE/norm\n",
    "\n",
    "\n",
    "#MSE as a function of time:\n",
    "ls_loss_fun_time=[]\n",
    "for t in range(reconstructed_comp.shape[-1]):\n",
    "    MSE=((image_all[:,:,t+1]-reconstructed_comp[:,:,t])**2).mean()\n",
    "\n",
    "    norm = (image_all[:,:,t+1]**2).mean()\n",
    "    ls_loss_fun_time.append(MSE/norm) \n",
    "\n",
    "\n",
    "np.save(\"/save path/image_all\", image_all)\n",
    "np.save(\"/save path/reconstructed_all_flipped\", reconstructed_all_flipped)\n",
    "np.save(\"/save path/mse_norm\", mse_norm)\n",
    "np.save(\"/save path/loss_fun_time\", np.array(ls_loss_fun_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template for plotting cochleagrams \n",
    "#Note 1: example for 3 images ids, but could plot more examples e.g., randomly selected ids to evaluate model reconstructions\n",
    "#Note 2: this is only for comparing original cochleagram with reconstructions from one model, but more columns can be added to integrate other models too (e.g., time scrambled)\n",
    "#Note 3: that the plotting code below would be similar for reconstruction of the last 30 time steps, except for small changes e.g., tick_labels = [0, 100, 200, 300, 400, 500, 600], ls_img[i][:,-30:] instead of ls_img[i][:,:]\n",
    "\n",
    "cochleagrams = image_all\n",
    "ae_reconstructions = np.load(\"/Users/ariellerosinski/My Drive/UCL/MSc/Project/Thesis/reconstruction_analyses/ae_reconstructed_all_flipped_3.npy\").squeeze()\n",
    "ae_reconstructions = np.swapaxes(ae_reconstructions, -1, -2)\n",
    "\n",
    "bird_id = 317\n",
    "frog_id = 53\n",
    "squirrel_id =903\n",
    "\n",
    "ls_img = [cochleagrams[bird_id],cochleagrams[frog_id], cochleagrams[squirrel_id]]\n",
    "ls_ae_recons = [ae_reconstructions[bird_id], ae_reconstructions[frog_id], ae_reconstructions[squirrel_id]]\n",
    "\n",
    "freq= [35.09722644, 50.0, 65.74421619, 82.37738465, 99.94969756, 118.51418101, 138.1268551, 158.84690291, 180.73684916, 203.86274881, 228.29438645, 254.10548686, 281.37393747, 310.1820234, 340.61667578, 372.76973403, 406.73822305, 442.62464596, 480.53729343, 520.59057046, 562.90534158, 607.60929561, 654.83733097, 704.73196269, 757.44375255, 813.13176334, 871.9640389, 934.11811118, 999.78153594, 1069.15245879, 1142.44021301, 1219.86595135, 1301.66331326, 1388.07913001, 1479.37416949, 1575.8239231, 1677.71943707, 1785.36819074, 1899.09502439, 2019.24311947, 2146.1750342, 2280.27379762, 2421.94406542, 2571.61334099, 2729.7332655, 2896.78098077, 3073.26056903, 3259.70457412, 3456.67560841, 3664.7680506, 3884.60983927, 4116.86436779, 4362.23248612, 4621.45461572, 4895.31298385, 5184.63398397, 5490.29066953, 5813.20538843, 6154.35256633, 6514.76164703, 6895.52019897, 7297.77719703, 7722.74648967, 8171.71046186, 8646.02390477, 9147.118104, 9676.50515859, 10235.78254397, 10826.63793245, 11450.85428595, 12110.31523625, 12807.01076902, 13543.04322881, 14320.63366307, 15142.12852432, 16010.00675089, 16926.88724725, 17895.53678687, 18918.87836118, 20000.0, 21142.16408994]\n",
    "ls_freq_2f=[]\n",
    "for value in freq:\n",
    "    ls_freq_2f.append(\"%.2f\" % value)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "n_plots=3\n",
    "fig, axs = plt.subplots(n_plots, 3, figsize=(15,8))\n",
    "fontsize=12\n",
    "for i in range(n_plots):\n",
    "    default_y_ticks = range(81)\n",
    "\n",
    "    img=axs[i, 0].imshow(ls_img[i][:,:],aspect=\"auto\", cmap='magma')                #could set vmin=0,vmax=0.7\n",
    "    axs[i, 0].axvline(x=70, color='red')\n",
    "    \n",
    "    recons_ae=axs[i, 1].imshow(ls_ae_recons[i][:,:],aspect=\"auto\", cmap='magma') \n",
    "    axs[i, 1].axvline(x=70, color='red')\n",
    "    \n",
    "    axs[i, 0].set_yticks(default_y_ticks,ls_freq_2f,fontsize=fontsize)\n",
    "    axs[i, 0].locator_params(axis='y', nbins=4)\n",
    "    axs[i, 0].set_ylabel(\"Frequency [Hz]\",fontsize=fontsize)\n",
    "    \n",
    "    if i == (n_plots-1):\n",
    "        default_xticks = range(99)\n",
    "        num_ticks = 5\n",
    "        tick_positions = np.linspace(0, len(default_xticks) - 1, num_ticks)\n",
    "        tick_labels = [0, 500, 1000, 1500, 2000]\n",
    "\n",
    "        axs[i, 0].set_xticks(tick_positions, tick_labels,fontsize=fontsize)\n",
    "        axs[i, 1].set_xticks(tick_positions, tick_labels,fontsize=fontsize)\n",
    "\n",
    "        axs[i, 0].set_xlabel(\"Time [ms]\",fontsize=fontsize)\n",
    "        axs[i, 1].set_xlabel(\"Time [ms]\",fontsize=fontsize)\n",
    "\n",
    "    \n",
    "        axs[i,0].tick_params(bottom=True, top=False, left=True, right=False,labelbottom=True, labeltop=False, labelleft=True, labelright=False)\n",
    "        axs[i,1].tick_params(bottom=True, top=False, left=False, right=False,labelbottom=True, labeltop=False, labelleft=False, labelright=False)\n",
    "    else:\n",
    "\n",
    "        axs[i,0].tick_params(bottom=False, top=False, left=True, right=False, labelbottom=False, labeltop=False, labelleft=True, labelright=False)\n",
    "        axs[i,1].tick_params(bottom=False, top=False, left=False, right=False, labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "\n",
    "cols = [\"Input cochleagram\", \"Reconstruction\"]\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(col,fontsize=13)\n",
    "plt.subplots_adjust(hspace=0.3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
